
hydra:
  run:
    dir: outputs_rl/auto_${now:%Y_%m_%d_%H_%M_%S}

  sweep:
    dir: outputs_rl/
    subdir: auto_${now:%Y_%m_%d_%H_%M_%S}_${hydra.job.num}

  output_subdir: "generate_config"

dataset_name: null # cbench,mibench
benchmark_exclude: null # comma separated keywords for exclusion, e.g., ghostscript,sort
benchmark_repeat: 80
seed: 0
get_stat: null # only query benchmarks statistics
load_benchmarks_from_json: null
json_key: null  # json key of the data split, e.g., 'test-small'
num_benchmarks: null  # if set, will try to use first `num_benchmarks` benchmarks
reset_best_return_on_every_episode: False  # if set true and benchmark_repeat > 1 and using the offline file (generate_utils.py), best return will be overwritten
online_update_vocab: False  # if True, update the common vocab on the fly if unknown token is encountered
graph_version: 1  # 0 for old graph type, 1 for new graph type

# Output db control
# if null, then we won't save db
outdir: null

vocab_db_path: "data/all_ssl_vocab.db"

# Run time control
patience: 30  # for offline data generation
runtime_per_job: null
max_step_per_job: 45
nproc: 80 # if null, use cpu_count()
max_episodes: null

# Models
model_path: null
gpu: cuda # null = cpu, or use a list (e.g. [0,1]) to specify multiple GPUs
eps: 0 # episilon greedy
T: 0  # Temperature used for sampling. If T = 0, then we do argmax
best_n: 1  # best n in A*
use_Astar: False  # use this option (instead of setting best_n) to control whether to use A*
use_AQ: False  # use the AQ* as in https://arxiv.org/pdf/2102.04518.pdf
use_policy: False

# how long to wait before flushing buffer into database
commit_frequency_in_seconds: 10
max_state_buffer_length: 200

# use submitit to send the tasks to other nodes instead of running locally
submitit:
  log_dir: null
  partition: learnlab
  timeout_min: 360
  jobs_per_task: 80
  cpus_per_task: 80
  mem_gb: 500
  gpus_per_node: 8
  constraint: volta32gb

traj_data: null  # read feather file and follow the trajectories therein, to replace offline random exploration

divided_by_this_ir: False  # for A* / AQ*, set this flag properly to get the correct estimation of future reward

# all parameters below are basically for online learning
generate_v4: true
device: cuda
traj_last_n: 5  # the number of last transitions to cut off
reward_discount: 0.9
model_db_path: ${outdir}/model.db  # seems not working
return_lower_bound: -1
n_model_workers: 8
use_autophase: False

# aggreate the jobs for a single forward pass in the neural network
model_capacity: 800000  # influence the waiting time for an item in the queue, either the #nodes (for graphs) or the batch size (for autophase)
load_full_rate: 0.6
job_full_rate: 0.5
wait_time: 0.1

load_model_frequency: 20
avg_last_n_scores: 100

min_ir: 100
max_ir: 10000
use_history: true
run_model_locally: true  # if true, the model is in the same process as the environment
GAE_lambda: 0.97  # the lambda for GAE-Lambda
use_ppo: true

norm_reward: false

eval_on_policy: false
model_rowid: null

# for online testing
online_test_json: null
online_test_max_step: 45
test_frequency_in_seconds: 900

# for creating socket that transfers data from generator to trainer
send_data_via_socket: true

use_only_anghabench: false  # for debugging purpose, only train on the Anghabench
traj_db: null  # the path of a db where action sequences of benchmarks are stored
for_reinforce: true  # use_ppo can overwrite this
pydantic_datasource: data/trajdataset_all10k-train-medium-all10k.json
pydantic_val_dataset_path: data/trajdataset_all10k-val-medium-all10k.json
pydantic_test_dataset_path: data/benchmarkdataset_all-test.json
simple_generation: false
early_stop_patience: 2
min_per_benchmark: 0.05
highest_reward: false